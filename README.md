Author/Owner: narsss1234
```
# Please-refer-to-the-file: ```assignment.py```
```

# Graded-Assignment-on-Monitoring-Scaling-and-Automation
Graded Assignment on Monitoring, Scaling and Automation

Develop a system that automatically manages the lifecycle of a web application hosted on  EC2 instances, monitors its health, and reacts to changes in traffic by scaling resources.  Furthermore, administrators receive notifications regarding the infrastructure's health and scaling events. 

```
I have already configured VPC, subnets and also security groups as using defaults generated by AWS and also the assignment did not require generating them.
```

Detailed Breakdown: 

1. Web Application Deployment: 

 - Use `boto3` to: 

 - Create an S3 bucket to store your web application's static files. 

 - Launch an EC2 instance and configure it as a web server (e.g., Apache, Nginx).  - Deploy the web application onto the EC2 instance. 

--> Solution
```
Created an S3 bucket client

Once the bucket is created, Uploaded the web application static file to S3.

Created an ec2 role manually and stored the role name - gave admin rights, as it will be access s3 and can be reused again for other instances if needed.

Created an ec2 client

by using userdata and other varaibles 

- created a t2.micro ec2

- addded shell script installed awscli, nginx

- Copied files from s3 and storede it locally

- moved the file to the nginx config files location and restarted nginx

--> once the instance is created storing the ec2 instance id, so that it can be used in future.

sleep for 100 seconds, so that the instance is up and ready, before the next task
```

2. Load Balancing with ELB: 

 - Deploy an Application Load Balancer (ALB) using `boto3`. 

 - Register the EC2 instance(s) with the ALB. 

--> Solution
```
Created an elbv2 client

using elbv2 client, created a target group, which is set to check as port 80, as nginx listens at port 80 by default

--> once target group is created, storing the target group arns in a list, to use it later

Using elbv2, created an ALB(application load balancer)

--> once alb is created, storing the alb arns in a list, to use it later
    with healthcheck at '/'

Using elbv2, register the instanceIds to the target group that was created.

Using elbv2, created a listener at port 80, type forward, and added to the existing alb

```

3. Auto Scaling Group (ASG) Configuration: 

 - Using `boto3`, create an ASG with the deployed EC2 instance as a template. 

 - Configure scaling policies to scale in/out based on metrics like CPU utilization or network traffic. 

 --> Solution
```
Created autoscaling client

- Created an autoscaling with the created ec2 instance as launch template/configuration.
-desired capacity - 1, min - 1, max - 2

Sleep for 2 mins, so that the autoscaling can be configured and the new instances are generated for the desired quantity.

Storing the arn for the autoscaling that was created.

-- created a Scaling out policy with scaling adjustment as 1

-- created a Scaling in policy with scaling adustment as -1
```

4. Lambda-based Health Checks & Management: 

 - Develop a Lambda function to periodically check the health of the web application  (through the ALB). 

 - If the health check fails consistently, the Lambda function should: 

 - Capture a snapshot of the failing instance for debugging purposes.

 - Terminate the problematic instance, allowing the ASG to replace it.  - Send a notification through SNS to the administrators. 

 --> Solution
```
Created a lambda code and stored it locally in 'lambda.py'

- The lambda code - uses boto3, to call elbv2, ec2, sns, and autosacling group

- It will check the health status of ec2 created by alb and this is retrieved from the target groups attached

- If any instance status is other than "healthy"

- It will call ec2 client to create snapshot of the ec2

- and then it will detach the instance from the autoscaling group, so that autoscaling group will cerate a new instance and attach it automactically.

- and then terminate the instance that was detached.

 --> once this is compeleted an SNS message will be pushed, with instanceId that got terminated to notify the administrators.
```
```
once the lambda is tested locally 

Created a new S3 bucket, to zip the lambda file and store it in the bucket

Once this is done

Created a lambda client

and created a lambda function, to fetch the code from s3, with a timeout 120 seconds, to avoid any additional costs for lambda execution for now, this can be increased based on the number of ec2 that will need to be checked

Created a cloudwatch 'events' client

Created a event with scheduled expression to run in a time interval

and later the target for the event to the lambda function ARN
```

5. S3 Logging & Monitoring: 

 - Configure the ALB to send access logs to the S3 bucket. 

 - Create a Lambda function that triggers when a new log is added to the S3 bucket. This function can analyze the log for suspicious activities (like potential DDoS attacks) or just high traffic. 

 - If any predefined criteria are met during the log analysis, the Lambda function sends a  notification via SNS. 

 ```
 Used elbv2 describe load balancer to get the arn for the alb and stored it in  a list

 Also created a new s3 bucket to store the logs from ALB

 
 ```

6. SNS Notifications: 

 - Set up different SNS topics for different alerts (e.g., health issues, scaling events, high traffic). 

 - Integrate SNS with Lambda so that administrators receive SMS or email notifications. 

7. Infrastructure Automation: 

 - Create a single script using `boto3` that: 

 - Deploys the entire infrastructure. 

 - Updates any component as required. 

 - Tears down everything when the application is no longer needed. 

8. Optional Enhancement â€“ Dynamic Content Handling: 

 - Store user-generated content or uploads on S3. 

 - When a user uploads content to the web application, it gets temporarily stored on the  EC2 instance. A background process (or another Lambda function) can move this to the S3  bucket and update the application's database to point to the content's new location on S3. 